{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WEB Scrapping Using BeautifulSoup webdriver "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# URL de la page Web que vous souhaitez extraire\n",
    "url = 'https://www.kooora.com/?n=0&o=n'\n",
    "\n",
    "# Utiliser Selenium WebDriver pour simuler un navigateur\n",
    "driver = webdriver.Chrome()  # Vous devez avoir installé le WebDriver de Chrome\n",
    "driver.get(url)\n",
    "\n",
    "# Attendre que la page soit complètement chargée\n",
    "wait = WebDriverWait(driver, 10)\n",
    "wait.until(EC.visibility_of_element_located((By.TAG_NAME, \"div\")))\n",
    "\n",
    "# Obtenir le contenu HTML complètement rendu\n",
    "html = driver.page_source\n",
    "\n",
    "# Fermer le WebDriver\n",
    "driver.quit()\n",
    "\n",
    "# Analyser le contenu HTML avec BeautifulSoup\n",
    "soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['الخالدية يتصدر الدوري البحريني مؤقتا', 'أحمد حمدي: جمهور الزمالك وفي وعظيم.. وهدفنا لقب الكونفدرالية', 'محمد شحاتة: جمهور الزمالك لا يدعمني بمفردي', 'استبعاد معلول وتاو من قائمة الأهلي ضد إنبي', 'مدرب فيوتشر يوجه طلبا للكاف بعد مغادرة الكونفيدرالية', 'صورة.. رحيمي يفقد وعيه في لقاء العين والنصر', 'بوكيتينو: لاعبو تشيلسي غير ناضجين', 'بعد التأهل الثالث.. ماذا قدم الزمالك في المربع الذهبي للكونفيدرالية؟', 'بالصور: الزمالك يعبر فيوتشر إلى نصف نهائي الكونفدرالية', 'هيثم فاروق: الزمالك يلعب دون مدرب', 'روي كين: أداء اليونايتد يليق بفرق وسط الجدول', 'جاياردو يستبعد ثلاثي اتحاد جدة', 'زيدان: نجم الريال استثنائي.. ويعجبني بشدة', 'حمدي يضع بصمته الأولى مع الزمالك', 'هل يكرس جيسوس عقدة كاسترو؟']\n",
      "['?n=1319172&pg=1&o=n', '?n=1319169&pg=1&o=n', '?n=1319171&pg=1&o=n', '?n=1319168&pg=1&o=n', '?n=1319170&pg=1&o=n', '?n=1319166&pg=1&o=n', '?n=1319158&pg=1&o=n', '?n=1319165&pg=1&o=n', '?n=1319164&pg=1&o=n', '?n=1319167&pg=1&o=n', '?n=1319160&pg=1&o=n', '?n=1319162&pg=1&o=n', '?n=1319062&pg=1&o=n', '?n=1319159&pg=1&o=n', '?n=1319148&pg=1&o=n']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Trouver tous les divs avec la classe \"newsList\"\n",
    "divs = soup.findAll('div', attrs={'class': \"newsList\"})\n",
    "\n",
    "# Initialisation d'une liste pour stocker les titres\n",
    "titles = []\n",
    "liens= []\n",
    "# Boucle pour parcourir chaque div\n",
    "for div in divs:\n",
    "    # Trouver tous les éléments li dans le div courant\n",
    "    li_elements = div.findAll('li')\n",
    "\n",
    "    # Pour chaque élément li, trouver tous les éléments strong et récupérer leur contenu textuel\n",
    "    for li in li_elements:\n",
    "        strong_elements = li.findAll('strong')\n",
    "        a_elements=li.findAll('a')\n",
    "        for strong,p in zip(strong_elements,a_elements):\n",
    "            # Ajouter le contenu textuel du strong à la liste des titres\n",
    "            titles.append(strong.text)\n",
    "            liens.append(p.get('href'))\n",
    "\n",
    "print(titles)\n",
    "print(liens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "contents = []\n",
    "for i in liens:\n",
    "    url = 'https://www.kooora.com/' + i\n",
    "\n",
    "    # Utiliser Selenium WebDriver pour simuler un navigateur\n",
    "    driver = webdriver.Chrome()  # Vous devez avoir installé le WebDriver de Chrome\n",
    "    driver.get(url)\n",
    "\n",
    "    # Attendre que la page soit complètement chargée\n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    wait.until(EC.visibility_of_element_located((By.TAG_NAME, \"div\")))\n",
    "\n",
    "    # Obtenir le contenu HTML complètement rendu\n",
    "    html = driver.page_source\n",
    "\n",
    "    # Fermer le WebDriver\n",
    "    driver.quit()\n",
    "    soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "    # Rechercher des balises <div> avec une classe spécifique\n",
    "    div_content = soup.findAll('div',attrs={'class':'articleBody'})\n",
    "    contents.append(div_content)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = [content[0].text for content in contents]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_content = [{'title':title , 'article' :article } for title, article in zip(titles,articles)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store the content in MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contenu JSON inséré avec succès dans MongoDB.\n"
     ]
    }
   ],
   "source": [
    "import pymongo\n",
    "import json\n",
    "\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "# Sélectionner une base de données\n",
    "db = client[\"demoDB\"]\n",
    "\n",
    "# Sélectionner une collection\n",
    "collection = db[\"sport\"]\n",
    "\n",
    "# Insérer le contenu JSON dans la collection\n",
    "collection.insert_many(json_content)\n",
    "\n",
    "print(\"Contenu JSON inséré avec succès dans MongoDB.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_str(text):\n",
    "\n",
    "#remove tashkeel\n",
    "    p_tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
    "    text = re.sub(p_tashkeel,'', text)\n",
    "    #remove longation\n",
    "    p_longation = re.compile(r'(.)\\1+')\n",
    "    subst = r\"\\1\\1\"\n",
    "    text = re.sub(p_longation, subst, text)\n",
    "    text = text.replace('و','وو')\n",
    "    text = text.replace('يي', 'ي')\n",
    "    text = text.replace('اا', 'ا')\n",
    "    text = text.replace('أ', 'ا')\n",
    "    text = text.replace('إ', 'ا')\n",
    "    text = text.replace('آ', 'ا')\n",
    "    text = text.replace('ى', 'ي')\n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    # Tokenization du texte\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    # Supprimer les mots vides\n",
    "    stop_words = set(stopwords.words('arabic'))\n",
    "    filtered_tokens = [token for token in tokens if token not in stop_words]\n",
    "    return filtered_tokens\n",
    "\n",
    "\n",
    "# Fonction pour normaliser les données\n",
    "def normalize_text(text):\n",
    "    text = clean_str(text)\n",
    "    tokens = tokenize_text(text)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Fonction pour discrétiser les données\n",
    "def discretize_text(text):\n",
    "    vectorizer = CountVectorizer()\n",
    "    vectorizer.fit_transform([text])\n",
    "    vocabulary = vectorizer.get_feature_names_out()\n",
    "    return vocabulary\n",
    "\n",
    "def pineLine(text):\n",
    "    text = normalize_text(text)\n",
    "    text = discretize_text(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the contents from MongoDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymongo\n",
    "\n",
    "client = pymongo.MongoClient(\"mongodb://localhost:27017/\")\n",
    "\n",
    "# Sélectionner une base de données\n",
    "db = client[\"demoDB\"]\n",
    "\n",
    "# Sélectionner une collection\n",
    "collection = db[\"sport\"]\n",
    "\n",
    "# Récupérer tous les documents de la collection\n",
    "documents = collection.find()\n",
    "\n",
    "# Stocker les documents dans une liste de dictionnaires\n",
    "contents=[document for document in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "contents_cleaning = [{'title':item['title'],'article':pineLine(item['article'])} for item in contents  ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stemming & lemmatization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Stemming and lemmatization are both techniques used in natural language processing (NLP) to reduce words to their base forms. However, they differ in their approach and the resulting output:\n",
    "\n",
    "### Stemming:\n",
    "\n",
    "- Simpler and faster approach.\n",
    "- Removes suffixes from words using predefined rules.\n",
    "- Results in a base form that might not be a real word (e.g., \"running\" stemmed to \"runn\").\n",
    "- Less accurate but computationally efficient.\n",
    "\n",
    "### Lemmatization:\n",
    "\n",
    "- More complex and slower approach.\n",
    "- Uses a dictionary and morphological analysis to identify the actual base word (lemma).\n",
    "- Results in a valid word (lemma) that represents the morphological root (e.g., \"running\" lemmatized to \"run\").\n",
    "- More accurate but requires more processing power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['اخري', 'اقيمت', 'ال', 'الاحد', 'الاخير', 'الاهداف', 'الاوول',\n",
       "       'البحرين', 'البحريني', 'البسيتين'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem.isri import ISRIStemmer\n",
    "def light_stem(contents_cleaning):\n",
    "    contents_stemming=[]\n",
    "    stemmer = ISRIStemmer()\n",
    "    for item in  contents_cleaning:\n",
    "        item_stem=[stemmer.stem(token) for token in item['article']]\n",
    "        contents_stemming.append({\"title\":item['title'],'article':item_stem})\n",
    "    return contents_stemming\n",
    "\n",
    "contents_stemming=light_stem(contents_cleaning)\n",
    "contents_cleaning[0]['article'][10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import qalsadi.lemmatizer\n",
    "\n",
    "def light_lemm(contents_cleaning):\n",
    "    contents_lemm=[]\n",
    "    lemmer = qalsadi.lemmatizer.Lemmatizer()\n",
    "    for item in  contents_cleaning:\n",
    "        item_stem=[lemmer.lemmatize(token) for token in item['article']]\n",
    "        contents_lemm.append({\"title\":item['title'],'article':item_stem})\n",
    "    return contents_stemming\n",
    "contents_lemm=light_lemm(contents_cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['اخر', 'اقم', 'ال', 'احد', 'اخر', 'هدف', 'اول', 'بحر', 'بحر', 'بسي']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents_lemm[0]['article'][10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NER Method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['11', '15', '17', '19', '22', '30', '31', '45', '86', 'احمد',\n",
       "       'اخري', 'اقيمت', 'ال', 'الاحد', 'الاخير', 'الاهداف', 'الاوول',\n",
       "       'البحرين', 'البحريني', 'البسيتين', 'التاسع', 'الترتيب', 'الثامن',\n",
       "       'الثاني', 'الجوولة', 'الحالة', 'الحد', 'الخالدية', 'الخسارة',\n",
       "       'الرابع', 'الرفاع', 'الرياضية', 'السابع', 'السادس', 'الشباب',\n",
       "       'الشرقاووي', 'الشرقي', 'الشيخ', 'العاشر', 'الفووز', 'الفيس',\n",
       "       'اللقاء', 'المباراة', 'المتذيل', 'المركز', 'المركزين', 'المسابقة',\n",
       "       'الممتاز', 'المنامة', 'النجمة', 'النقطة', 'الهدف', 'الووحيد',\n",
       "       'الووطني', 'الي', 'امير', 'ان', 'بالخطا', 'بالدقيقة', 'بجدوول',\n",
       "       'بفارق', 'بن', 'بنتيجة', 'بهدف', 'بهذا', 'ترتيب', 'تووباريس',\n",
       "       'تووقف', 'جاء', 'جانبه', 'جمعتهما', 'حمد', 'خلال', 'خليفة',\n",
       "       'دقائق', 'دووري', 'رصيد', 'رصيده', 'رفع', 'رووستائي', 'ستاد',\n",
       "       'سترة', 'سجل', 'سجله', 'صاحب', 'صاحبي', 'صدارة', 'طريق', 'علي',\n",
       "       'فاز', 'فاسيووندوو', 'فرانسيسكوو', 'فلامرزي', 'في', 'لاعب',\n",
       "       'لاعبه', 'لجدوول', 'لفريق', 'للحد', 'للمنامة', 'مؤقتة', 'مارلي',\n",
       "       'مباراة', 'محمد', 'مدينة', 'مرماه', 'مروور', 'من', 'ناصر', 'نقطة',\n",
       "       'نقطتين', 'نيلسوون', 'هدف', 'ووالبسيتين', 'ووالتاسع', 'ووبفارق',\n",
       "       'ووبنقطة', 'ووتقدم', 'ووتووقف', 'وورفع', 'يتعادل'], dtype=object)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "contents_cleaning[0]['article']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\dell\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('11', 'CD'),\n",
       " ('15', 'CD'),\n",
       " ('17', 'CD'),\n",
       " ('19', 'CD'),\n",
       " ('22', 'CD'),\n",
       " ('30', 'CD'),\n",
       " ('31', 'CD'),\n",
       " ('45', 'CD'),\n",
       " ('86', 'CD'),\n",
       " ('احمد', 'NNP'),\n",
       " ('اخري', 'DT'),\n",
       " ('اقيمت', 'VBD'),\n",
       " ('ال', 'DT'),\n",
       " ('الاحد', 'NNP'),\n",
       " ('الاخير', 'NN'),\n",
       " ('الاهداف', 'NNP'),\n",
       " ('الاوول', 'NNP'),\n",
       " ('البحرين', 'VBD'),\n",
       " ('البحريني', 'NNP'),\n",
       " ('البسيتين', 'NNP'),\n",
       " ('التاسع', 'NNP'),\n",
       " ('الترتيب', 'NNP'),\n",
       " ('الثامن', 'NNP'),\n",
       " ('الثاني', 'NN'),\n",
       " ('الجوولة', 'NNP'),\n",
       " ('الحالة', 'NN'),\n",
       " ('الحد', 'NN'),\n",
       " ('الخالدية', 'NNP'),\n",
       " ('الخسارة', 'NN'),\n",
       " ('الرابع', 'NN'),\n",
       " ('الرفاع', 'NNP'),\n",
       " ('الرياضية', 'NNP'),\n",
       " ('السابع', 'NNP'),\n",
       " ('السادس', 'NNP'),\n",
       " ('الشباب', 'NNP'),\n",
       " ('الشرقاووي', 'NNP'),\n",
       " ('الشرقي', 'NNP'),\n",
       " ('الشيخ', 'NNP'),\n",
       " ('العاشر', 'NNP'),\n",
       " ('الفووز', 'NNP'),\n",
       " ('الفيس', 'NNP'),\n",
       " ('اللقاء', 'NNP'),\n",
       " ('المباراة', 'VB'),\n",
       " ('المتذيل', 'NN'),\n",
       " ('المركز', 'NNP'),\n",
       " ('المركزين', 'NNP'),\n",
       " ('المسابقة', 'NNP'),\n",
       " ('الممتاز', 'NNP'),\n",
       " ('المنامة', 'NNP'),\n",
       " ('النجمة', 'NN'),\n",
       " ('النقطة', 'NN'),\n",
       " ('الهدف', 'NN'),\n",
       " ('الووحيد', 'NNP'),\n",
       " ('الووطني', 'NNP'),\n",
       " ('الي', 'TO'),\n",
       " ('امير', 'NNP'),\n",
       " ('ان', 'IN'),\n",
       " ('بالخطا', 'NN'),\n",
       " ('بالدقيقة', 'NNP'),\n",
       " ('بجدوول', 'NNP'),\n",
       " ('بفارق', 'NNP'),\n",
       " ('بن', 'NN'),\n",
       " ('بنتيجة', 'NNP'),\n",
       " ('بهدف', 'NNP'),\n",
       " ('بهذا', 'NN'),\n",
       " ('ترتيب', 'NN'),\n",
       " ('تووباريس', 'NNP'),\n",
       " ('تووقف', 'NNP'),\n",
       " ('جاء', 'VBD'),\n",
       " ('جانبه', 'RB'),\n",
       " ('جمعتهما', 'NNP'),\n",
       " ('حمد', 'VBD'),\n",
       " ('خلال', 'IN'),\n",
       " ('خليفة', 'NN'),\n",
       " ('دقائق', 'NNS'),\n",
       " ('دووري', 'NN'),\n",
       " ('رصيد', 'VBP'),\n",
       " ('رصيده', 'NNP'),\n",
       " ('رفع', 'NNP'),\n",
       " ('رووستائي', 'NNP'),\n",
       " ('ستاد', 'NN'),\n",
       " ('سترة', 'NNP'),\n",
       " ('سجل', 'NN'),\n",
       " ('سجله', 'NNP'),\n",
       " ('صاحب', 'NN'),\n",
       " ('صاحبي', 'VBP'),\n",
       " ('صدارة', 'JJ'),\n",
       " ('طريق', 'NN'),\n",
       " ('علي', 'IN'),\n",
       " ('فاز', 'NNP'),\n",
       " ('فاسيووندوو', 'NNP'),\n",
       " ('فرانسيسكوو', 'NN'),\n",
       " ('فلامرزي', 'NNP'),\n",
       " ('في', 'IN'),\n",
       " ('لاعب', 'NN'),\n",
       " ('لاعبه', 'NNP'),\n",
       " ('لجدوول', 'NNP'),\n",
       " ('لفريق', 'NNP'),\n",
       " ('للحد', 'NNP'),\n",
       " ('للمنامة', 'VBP'),\n",
       " ('مؤقتة', 'JJ'),\n",
       " ('مارلي', 'NNP'),\n",
       " ('مباراة', 'NN'),\n",
       " ('محمد', 'NNP'),\n",
       " ('مدينة', 'NN'),\n",
       " ('مرماه', 'NNP'),\n",
       " ('مروور', 'NNP'),\n",
       " ('من', 'IN'),\n",
       " ('ناصر', 'NN'),\n",
       " ('نقطة', 'NN'),\n",
       " ('نقطتين', 'NNP'),\n",
       " ('نيلسوون', 'NN'),\n",
       " ('هدف', 'NNP'),\n",
       " ('ووالبسيتين', 'NNP'),\n",
       " ('ووالتاسع', 'NN'),\n",
       " ('ووبفارق', 'NNP'),\n",
       " ('ووبنقطة', 'NNP'),\n",
       " ('ووتقدم', 'NNP'),\n",
       " ('ووتووقف', 'VB'),\n",
       " ('وورفع', 'NN'),\n",
       " ('يتعادل', 'IN')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('words')\n",
    "sentence = \"مرحبا بالعالم!\"\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "traitement = pos_tag([GoogleTranslator(source='auto', target='en').translate(item) for item in contents_cleaning[0]['article']])\n",
    "traitement=resultat = [x[1] for x in traitement]\n",
    "pos_rules = [(mot,role) for mot , role in zip(contents_cleaning[0]['article'],traitement)]\n",
    "pos_rules\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "فاز الخالدية على البسيتين (1-0)، مساء الأحد، على ستاد البحرين الوطني، في الجولة 15 من دوري ناصر بن حمد البحريني الممتاز.جاء هدف اللقاء الوحيد بعد مرور 7 دقائق عن طريق لاعب الخالدية علي الشرقاوي.ورفع الخالدية رصيده بهذا الفوز إلى 30 نقطة في صدارة مؤقتة لجدول ترتيب المسابقة بفارق نقطتين عن الرفاع صاحب المركز الثاني.وتوقف رصيد البسيتين عند النقطة 15 في المركز التاسع بفارق الأهداف عن الرفاع الشرقي صاحب المركز العاشر وبنقطة عن النجمة صاحب المركز الثامن.في مباراة أخرى فاز المنامة بنتيجة (2-1) على سترة خلال المباراة التي أُقيمت على ستاد مدينة خليفة الرياضية.وتقدم المنامة بعد مرور 5 دقائق بهدف سجله لاعبه ألفيس مارلي قبل أن يتعادل أمير روستائي لفريق سترة بالدقيقة 8، ثم سجل فاسيوندو توباريس الهدف الثاني للمنامة بالدقيقة 45.ورفع المنامة رصيده بهذا الفوز إلى النقطة 22 في المركز الرابع بجدول الترتيب، فيما توقف رصيد سترة عند 17 نقطة في المركز السابع.من جانبه فاز الحد بنتيجة (2-0) على الحالة خلال المباراة التي جمعتهما على ستاد الشيخ علي بن محمد آل خليفة.جاء الهدف الأول للحد عن طريق لاعب الحالة فرانسيسكو نيلسون بالخطأ في مرماه بالدقيقة 31 فيما سجل الهدف الثاني أحمد فلامرزي بالدقيقة 86.الفوز رفع رصيد الحد إلى النقطة 15 في المركز 11 ـ قبل الأخير ـ بفارق نقطة عن الشباب المتذيل وبفارق الأهداف عن الرفاع الشرقي والبسيتين صاحبي المركزين العاشر والتاسع.وتوقف رصيد الحالة بعد الخسارة عند النقطة 19 في المركز السادس.\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Charger le modèle pour l'arabe\n",
    "nlp = spacy.load(\"xx_ent_wiki_sm\")\n",
    "\n",
    "texte_arabe=contents[0]['article']\n",
    "print(texte_arabe)\n",
    "\n",
    "# Traiter le texte avec spaCy\n",
    "doc = nlp(texte_arabe)\n",
    "\n",
    "# Afficher les entités nommées\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using spaCy for processing Arabic text, especially for tasks like Named Entity Recognition (NER), presents several challenges due to the unique characteristics of the Arabic language. Here are some of the key issues:\n",
    "- Limited Language Resources: Compared to languages like English, Arabic has fewer available language resources, such as annotated corpora, pre-trained models, and linguistic tools. This scarcity of resources can hinder the development and performance of NER systems for Arabic.\n",
    "- Named Entity Variation: Named entities in Arabic can vary significantly in form, especially when considering the various ways in which names can be represented (e.g., full names, titles, honorifics). Additionally, Arabic named entities often include multi-word expressions, which can further complicate detection and classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
